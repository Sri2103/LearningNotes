SGD → stochastic gradient descent  
Batchnorm → normalize batch inputs  
Overfitting → memorize training data  
Underfitting → model too simple  
Loss function → objective to minimize  
Embedding → dense vector representation  
Transformer → attention-based architecture
